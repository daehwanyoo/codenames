{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FastText**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText is a word embedding model developed by Facebook AI Research that extends the Word2Vec model by considering subword information, allowing it to handle out-of-vocabulary words and capture morphological similarities. It breaks words into smaller subword units called character n-grams, enabling it to generate embeddings for rare or unseen words and better represent the semantics of words in a given text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://spotintelligence.com/2023/12/05/fasttext/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/pwqbdz21323f3j79799_2x7m0000gn/T/ipykernel_6806/2525775472.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import gensim\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import nltk\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the cleaned dictionary\n",
    "df = pd.read_csv('cleaned_dict.csv')\n",
    "\n",
    "#replace commas with spaces\n",
    "df['definition'] = df['definition'].str.replace(',', ' ')\n",
    "#remove any non-alphabetic characters\n",
    "df['definition'] = df['definition'].str.replace('[^a-zA-Z]', '')\n",
    "\n",
    "#tokenize the definitions\n",
    "df['definition'] = df['definition'].apply(word_tokenize)\n",
    "#remove quotes from the words\n",
    "df['definition'] = df['definition'].apply(lambda x: [word.replace(\"'\", \"\") for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11590859, 25258100)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Train FastText model\n",
    "# TODO: Adjust the hyperparameters of the FastText model; the baseline model isn't good!\n",
    "model = FastText(vector_size=100,\n",
    "                                window=5,\n",
    "                                min_count=2,\n",
    "                                workers=4,\n",
    "                                sg=1,\n",
    "                                epochs=10,\n",
    "                                min_n=2,         # Smaller n-grams to capture more subword details\n",
    "                                max_n=8)         # Larger n-grams for more context\n",
    "\n",
    "model.build_vocab(corpus_iterable=df['definition'])\n",
    "model.train(corpus_iterable=df['definition'], total_examples=len(df['definition']), epochs=model_subword_tuning.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'ambulance': [('valance', 0.7999069690704346), ('avalanche', 0.7785946130752563), ('lance', 0.7740586996078491), ('lancelet', 0.7724210619926453), ('lancet', 0.7722228765487671), ('ambuscade', 0.7539688944816589), ('askance', 0.7527194023132324), ('elance', 0.7487396597862244), ('balance', 0.7485995292663574), ('parlance', 0.7449723482131958)]\n"
     ]
    }
   ],
   "source": [
    "# Example: Find similar words to 'king'\n",
    "similar_words = model.wv.most_similar(\"ambulance\", topn=10)\n",
    "print(\"Similar words to 'ambulance':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to: ['JAM', 'DWARF']\n",
      "[('pudgy', 0.2768684923648834), ('puffy', 0.27478259801864624), ('scurfy', 0.26838046312332153), ('scabby', 0.2654760181903839), ('scuff', 0.26016759872436523)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "with open('wordlist-eng.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "random_words = random.sample(words, 2)\n",
    "similar_words = model.wv.most_similar(random_words, topn=5)\n",
    "print(\"Similar words to:\", random_words)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('princeling', 0.807127058506012), ('kingship', 0.7892905473709106), ('kingcup', 0.7537573575973511), ('seignior', 0.7436124086380005), ('marquee', 0.7380726337432861)]\n"
     ]
    }
   ],
   "source": [
    "words = ['king', 'queen', 'prince']\n",
    "similar_words = model.wv.most_similar(words, topn=5)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task: Improve the model, create visualizations, compare findings to our baseline Word2Vec model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
